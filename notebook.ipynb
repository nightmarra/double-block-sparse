{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e52794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajno\\Desktop\\Thesis\\block_sparsity\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 120.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8324509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "lm_head.weight torch.Size([128256, 4096])\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if \"proj\" in name or \"lm_head\" in name:\n",
    "        print(name, param.shape)\n",
    "        length += 1\n",
    "print(length)\n",
    "\n",
    "# from double_sparse.modelutils import find_layers\n",
    "# a = find_layers(model)\n",
    "# for i in a:\n",
    "#     print(i)\n",
    "# print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4969a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0092,  0.0076,  0.0015,  ...,  0.0166,  0.0034,  0.0076],\n",
      "        [ 0.0125,  0.0153,  0.0129,  ..., -0.0004,  0.0128,  0.0056],\n",
      "        [ 0.0192,  0.0037, -0.0167,  ...,  0.0115,  0.0078, -0.0008],\n",
      "        ...,\n",
      "        [-0.0164,  0.0254,  0.0026,  ...,  0.0089, -0.0062,  0.0029],\n",
      "        [-0.0457,  0.0171, -0.0064,  ..., -0.0184, -0.0020, -0.0018],\n",
      "        [-0.0135, -0.0003, -0.0234,  ..., -0.0082,  0.0088,  0.0267]],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "q_proj_weight = model.model.layers[3].self_attn.q_proj.weight\n",
    "# q_proj_weight = model.model.layers[0].self_attn.k_proj.weight\n",
    "print(q_proj_weight.shape)   # torch.Size([4096, 4096])\n",
    "print(q_proj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe9d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doublesparse import factorizef, mag_prune\n",
    "from ignite.metrics.regression.mean_absolute_relative_error import MeanAbsoluteRelativeError\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b5d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "q_proj_weight = q_proj_weight.cuda()\n",
    "print(q_proj_weight.device)\n",
    "print(q_proj_weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac1956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_factorize(mask=None):\n",
    "    torch.cuda.synchronize()\n",
    "    SIZE = 4096\n",
    "    \n",
    "    matrix = q_proj_weight.to(dtype=torch.float32)\n",
    "    identity = torch.eye(SIZE, device=\"cuda\")\n",
    "    prod, A, B = factorizef(matrix, identity, fixmask=mask)\n",
    "\n",
    "    frobenius = torch.norm(prod - matrix, p='fro')\n",
    "    print(f\"Frobenius norm: {frobenius.item()}\")\n",
    "\n",
    "    prod = prod.cpu()\n",
    "    A = A.cpu()\n",
    "    B = B.cpu()\n",
    "\n",
    "    nz_count_A = torch.count_nonzero(A).item()\n",
    "    nz_count_B = torch.count_nonzero(B).item()\n",
    "\n",
    "    print(f\"A: {A[:4, :4]}\")\n",
    "    print(f\"B: {B[:4, :4]}\")\n",
    "    print(f\"AB: {prod}\")\n",
    "    print(f\"Input matrix was: {matrix}\")\n",
    "\n",
    "    del A, B, matrix, identity\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"A has {nz_count_A} non-zero entries ({round(nz_count_A/(SIZE**2)*100, 1)}%)\")\n",
    "    print(f\"B has {nz_count_B} non-zero entries ({round(nz_count_B/(SIZE**2)*100, 1)}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f11a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_mag_prune(mask=None):\n",
    "    torch.cuda.synchronize()\n",
    "    SIZE = 4096\n",
    "    \n",
    "    matrix = q_proj_weight.to(dtype=torch.float32)\n",
    "    identity = torch.eye(SIZE, device=\"cuda\")\n",
    "    prod = mag_prune(matrix)\n",
    "\n",
    "    frobenius = torch.norm(matrix - prod, p='fro')\n",
    "    print(f\"Frobenius norm: {frobenius.item()}\")\n",
    "\n",
    "    prod = prod.cpu()\n",
    "    print(f\"AB: {prod}\")\n",
    "    print(f\"Input matrix was: {matrix}\")\n",
    "\n",
    "    del matrix, identity\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4512de86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 11.107711791992188\n",
      "A: tensor([[-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000, -0.0159],\n",
      "        [ 0.0444,  0.0000, -0.0000,  0.0000]])\n",
      "B: tensor([[0.5037, 0.0000, 0.0256, -0.0000],\n",
      "        [0.0000, 0.4861, 0.0000, -0.0000],\n",
      "        [-0.0000, 0.0000, 0.4536, 0.0000],\n",
      "        [-0.0000, 0.0000, -0.0000, 0.4842]])\n",
      "AB: tensor([[ 0.0110,  0.0053, -0.0034,  ...,  0.0195,  0.0077,  0.0048],\n",
      "        [ 0.0144,  0.0151,  0.0122,  ...,  0.0012,  0.0163,  0.0088],\n",
      "        [ 0.0154,  0.0084, -0.0134,  ...,  0.0136,  0.0020,  0.0028],\n",
      "        ...,\n",
      "        [-0.0203,  0.0232,  0.0048,  ...,  0.0073, -0.0018,  0.0028],\n",
      "        [-0.0429,  0.0223, -0.0047,  ..., -0.0177, -0.0004, -0.0025],\n",
      "        [-0.0090,  0.0005, -0.0237,  ..., -0.0118,  0.0090,  0.0314]])\n",
      "Input matrix was: tensor([[ 0.0092,  0.0076,  0.0015,  ...,  0.0166,  0.0034,  0.0076],\n",
      "        [ 0.0125,  0.0153,  0.0129,  ..., -0.0004,  0.0128,  0.0056],\n",
      "        [ 0.0192,  0.0037, -0.0167,  ...,  0.0115,  0.0078, -0.0008],\n",
      "        ...,\n",
      "        [-0.0164,  0.0254,  0.0026,  ...,  0.0089, -0.0062,  0.0029],\n",
      "        [-0.0457,  0.0171, -0.0064,  ..., -0.0184, -0.0020, -0.0018],\n",
      "        [-0.0135, -0.0003, -0.0234,  ..., -0.0082,  0.0088,  0.0267]],\n",
      "       device='cuda:0')\n",
      "A has 4026531 non-zero entries (24.0%)\n",
      "B has 2684353 non-zero entries (16.0%)\n"
     ]
    }
   ],
   "source": [
    "test_factorize(mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5f3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 22.477331161499023\n",
      "AB: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0166,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0153,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0192,  0.0000, -0.0167,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        ...,\n",
      "        [-0.0164,  0.0254,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0457,  0.0171, -0.0000,  ..., -0.0184, -0.0000, -0.0000],\n",
      "        [-0.0135, -0.0000, -0.0234,  ..., -0.0000,  0.0000,  0.0267]])\n",
      "Input matrix was: tensor([[ 0.0092,  0.0076,  0.0015,  ...,  0.0166,  0.0034,  0.0076],\n",
      "        [ 0.0125,  0.0153,  0.0129,  ..., -0.0004,  0.0128,  0.0056],\n",
      "        [ 0.0192,  0.0037, -0.0167,  ...,  0.0115,  0.0078, -0.0008],\n",
      "        ...,\n",
      "        [-0.0164,  0.0254,  0.0026,  ...,  0.0089, -0.0062,  0.0029],\n",
      "        [-0.0457,  0.0171, -0.0064,  ..., -0.0184, -0.0020, -0.0018],\n",
      "        [-0.0135, -0.0003, -0.0234,  ..., -0.0082,  0.0088,  0.0267]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_mag_prune(mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06dcd2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 51.09311294555664\n",
      "A: tensor([[ 0.0000,  0.0081, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0185,  0.0135, -0.0000, -0.0098]])\n",
      "B: tensor([[ 0.7362, -0.0020,  0.0396, -0.0077],\n",
      "        [-0.0203,  0.7104,  0.0035,  0.0284],\n",
      "        [-0.0092,  0.0020,  0.7983,  0.0492],\n",
      "        [ 0.0215,  0.0018,  0.0504,  0.7454]])\n",
      "AB: tensor([[ 0.0103,  0.0104,  0.0024,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0119,  0.0131,  0.0107,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0200,  0.0011, -0.0178,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0174,  0.0235,  0.0026,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0439,  0.0166, -0.0084,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0129,  0.0006, -0.0227,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Input matrix was: tensor([[ 0.0092,  0.0076,  0.0015,  ...,  0.0166,  0.0034,  0.0076],\n",
      "        [ 0.0125,  0.0153,  0.0129,  ..., -0.0004,  0.0128,  0.0056],\n",
      "        [ 0.0192,  0.0037, -0.0167,  ...,  0.0115,  0.0078, -0.0008],\n",
      "        ...,\n",
      "        [-0.0164,  0.0254,  0.0026,  ...,  0.0089, -0.0062,  0.0029],\n",
      "        [-0.0457,  0.0171, -0.0064,  ..., -0.0184, -0.0020, -0.0018],\n",
      "        [-0.0135, -0.0003, -0.0234,  ..., -0.0082,  0.0088,  0.0267]],\n",
      "       device='cuda:0')\n",
      "A has 2516581 non-zero entries (15.0%)\n",
      "B has 4192256 non-zero entries (25.0%)\n"
     ]
    }
   ],
   "source": [
    "dblock_upper_half = torch.cat((torch.ones([32, 32], device=\"cuda\"), torch.zeros([32, 32], device=\"cuda\")), dim=1)\n",
    "dblock_lower_half = torch.zeros([32, 64], device=\"cuda\")\n",
    "dblock = torch.cat((dblock_upper_half, dblock_lower_half), dim=0)\n",
    "for _ in range(6):\n",
    "    dblock = torch.cat((dblock, dblock), dim=1)\n",
    "for _ in range(6):\n",
    "    dblock = torch.cat((dblock, dblock), dim=0)\n",
    "\n",
    "test_factorize(mask=dblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68281f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_sparsity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
